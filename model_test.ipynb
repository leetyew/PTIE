{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1af0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ViT_bidirectional_dualPatches_untie_inference(\n",
       "    (linear_4by8): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (linear_8by4): Linear(in_features=32, out_features=512, bias=True)\n",
       "    (image_enc): FeaturesEncoder_dualPatches_untie(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer_untie(\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer_untie(\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer_untie(\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer_untie(\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer_untie(\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer_untie(\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (_4by8_pe): Embedding(128, 512)\n",
       "      (pe_q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (pe_k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (_8by4_pe): Embedding(128, 512)\n",
       "    )\n",
       "    (image_dec): TransformerDecoder_bidirectional_dualPatches_untie(\n",
       "      (embedding): OrthoEmbedding_bidirectional()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer_untie(\n",
       "          (maskedAttention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "          (norm_3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer_untie(\n",
       "          (maskedAttention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "          (norm_3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer_untie(\n",
       "          (maskedAttention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "          (norm_3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer_untie(\n",
       "          (maskedAttention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "          (norm_3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer_untie(\n",
       "          (maskedAttention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "          (norm_3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer_untie(\n",
       "          (maskedAttention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention_untie(\n",
       "            (q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
       "          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (feedForward): FeedForward(\n",
       "            (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout_3): Dropout(p=0.1, inplace=False)\n",
       "          (norm_3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (LR_pe): Embedding(30, 512)\n",
       "      (RL_pe): Embedding(30, 512)\n",
       "      (pe_q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (pe_k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (enc_k_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dec_q_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import models.models as models\n",
    "import utils.configuration as Conf\n",
    "import numpy as np\n",
    "import data\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import pickle\n",
    "import utils.process_samples as Process\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "config = Conf.Config()\n",
    "config.update_options_from_path('configs/train.yaml')\n",
    "\n",
    "MODEL_NAME = 'ViT_bidirectional_dualPatches_untie_inference'\n",
    "TRANSFORM = config.val_dataset_options['transform']\n",
    "MAX_SEQ_LEN = config.model_options['dec_seq_len']\n",
    "MULTIPLE_GPU = config.base_options['multi_gpu']\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config.model_options['in_num_embeddings'] = 100\n",
    "config.model_options['out_num_embeddings'] = 100\n",
    "config.model_options['enc_dropout'] = 0.1\n",
    "config.model_options['dec_dropout'] = 0.1\n",
    "config.model_options['enc_seq_len'] = 128\n",
    "\n",
    "if MULTIPLE_GPU:\n",
    "    model = getattr(models, MODEL_NAME)(config.model_options).to(DEVICE)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.load_state_dict(torch.load('./best_avg.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94bed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc LR 4by8:  0.8854166666666666\n",
      "Acc RL 4by8:  0.8923611111111112\n",
      "Acc LR 8by4:  0.90625\n",
      "Acc RL 8by4:  0.8854166666666666\n",
      "count:  288\n",
      "\n",
      "Highest Prob in LR 4by8: 76\n",
      "Highest Prob in RL 4by8: 50\n",
      "Highest Prob in LR 8by4: 80\n",
      "Highest Prob in RL 8by4: 82\n",
      "Acc: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cute80Dataset_Custom'\n",
    "\n",
    "model.eval()\n",
    "model.reqiures_grad = False\n",
    "single_case = True\n",
    "with_sym = False\n",
    "\n",
    "corpus = config.corpus\n",
    "\n",
    "GT_text_RL = []\n",
    "pred_text_RL_4by8 = []\n",
    "prob_list_RL_4by8 = []\n",
    "pred_text_RL_8by4 = []\n",
    "prob_list_RL_8by4 = []\n",
    "\n",
    "GT_text_LR = []\n",
    "pred_text_LR_4by8 = []\n",
    "prob_list_LR_4by8 = []\n",
    "pred_text_LR_8by4 = []\n",
    "prob_list_LR_8by4 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_count = 0\n",
    "    val_acc_word_LR_4by8 = 0\n",
    "    val_acc_word_RL_4by8 = 0\n",
    "    val_acc_word_LR_8by4 = 0\n",
    "    val_acc_word_RL_8by4 = 0\n",
    "\n",
    "    testDataset = getattr(data, dataset)(**config.test_dataset_options)\n",
    "    testDataset.pad = False\n",
    "    testDataset.width = 128\n",
    "    testDataset.out_channel = 1\n",
    "    loader = torch.utils.data.DataLoader(testDataset, batch_size=640, shuffle=False)\n",
    "\n",
    "    temp_pred_bool_arr_LR_4by8 = np.zeros((testDataset.__len__(),))\n",
    "    temp_pred_bool_arr_RL_4by8 = np.zeros((testDataset.__len__(),))\n",
    "    temp_pred_bool_arr_LR_8by4 = np.zeros((testDataset.__len__(),))\n",
    "    temp_pred_bool_arr_RL_8by4 = np.zeros((testDataset.__len__(),))        \n",
    "    \n",
    "    counter = 0\n",
    "    for test_output in loader:\n",
    "        image_4by8 =  test_output[0]\n",
    "        image_8by4 =  test_output[1]\n",
    "        text_LR = test_output[2]\n",
    "        text_RL = test_output[3]\n",
    "\n",
    "        image_4by8 = image_4by8.cuda()\n",
    "        image_8by4 = image_8by4.cuda()\n",
    "        bs = image_4by8.size(0)\n",
    "\n",
    "        val_count += image_4by8.size(0)\n",
    "        output = Process.word_level_acc_bidirectional_dualPatches_untie_inference(model,image_4by8, image_8by4, text_LR, text_RL, corpus, MAX_SEQ_LEN, DEVICE,True, single_case, with_sym)\n",
    "\n",
    "        temp_acc_LR_4by8 = output[0]\n",
    "        temp_acc_RL_4by8 = output[1]\n",
    "        temp_acc_LR_8by4 = output[2]\n",
    "        temp_acc_RL_8by4 = output[3]     \n",
    "\n",
    "        temp_pred_bool_arr_LR_4by8[counter:counter+bs] = output[4]\n",
    "        temp_pred_bool_arr_RL_4by8[counter:counter+bs] = output[5]\n",
    "        temp_pred_bool_arr_LR_8by4[counter:counter+bs] = output[6]\n",
    "        temp_pred_bool_arr_RL_8by4[counter:counter+bs] = output[7]\n",
    "\n",
    "        counter += bs\n",
    "        val_acc_word_LR_4by8 += temp_acc_LR_4by8\n",
    "        val_acc_word_RL_4by8 += temp_acc_RL_4by8\n",
    "        val_acc_word_LR_8by4 += temp_acc_LR_8by4\n",
    "        val_acc_word_RL_8by4 += temp_acc_RL_8by4\n",
    "\n",
    "        GT_text_LR += output[8]\n",
    "        GT_text_RL += output[9]\n",
    "        pred_text_LR_4by8 += output[10]\n",
    "        pred_text_RL_4by8 += output[11]\n",
    "        pred_text_LR_8by4 += output[12]\n",
    "        pred_text_RL_8by4 += output[13] \n",
    "\n",
    "        output_prob_LR_4by8 = output[14] \n",
    "        output_prob_RL_4by8 = output[15] \n",
    "        output_prob_LR_8by4 = output[16] \n",
    "        output_prob_RL_8by4 = output[17] \n",
    "\n",
    "        output_prob_LR_4by8, _ = torch.max(output_prob_LR_4by8, dim=2)\n",
    "        output_prob_RL_4by8, _ = torch.max(output_prob_RL_4by8, dim=2)\n",
    "        output_prob_LR_8by4, _ = torch.max(output_prob_LR_8by4, dim=2)\n",
    "        output_prob_RL_8by4, _ = torch.max(output_prob_RL_8by4, dim=2)\n",
    "\n",
    "        for i in range(bs):\n",
    "            text_prob_LR_4by8 = 1\n",
    "            for j in range(len(output[10][i])):\n",
    "                text_prob_LR_4by8 *= output_prob_LR_4by8[i, j].item()\n",
    "            prob_list_LR_4by8.append(text_prob_LR_4by8)\n",
    "        for i in range(bs):\n",
    "            text_prob_RL_4by8 = 1\n",
    "            for j in range(len(output[11][i])):\n",
    "                text_prob_RL_4by8 *= output_prob_RL_4by8[i, j].item()\n",
    "            prob_list_RL_4by8.append(text_prob_RL_4by8)\n",
    "        for i in range(bs):\n",
    "            text_prob_LR_8by4 = 1\n",
    "            for j in range(len(output[12][i])):\n",
    "                text_prob_LR_8by4 *= output_prob_LR_8by4[i, j].item()\n",
    "            prob_list_LR_8by4.append(text_prob_LR_8by4)\n",
    "        for i in range(bs):\n",
    "            text_prob_RL_8by4 = 1\n",
    "            for j in range(len(output[13][i])):\n",
    "                text_prob_RL_8by4 *= output_prob_RL_8by4[i, j].item()\n",
    "            prob_list_RL_8by4.append(text_prob_RL_8by4)\n",
    "            \n",
    "    \n",
    "    print('Acc LR 4by8: ', val_acc_word_LR_4by8/val_count)\n",
    "    print('Acc RL 4by8: ', val_acc_word_RL_4by8/val_count)\n",
    "    print('Acc LR 8by4: ', val_acc_word_LR_8by4/val_count)\n",
    "    print('Acc RL 8by4: ', val_acc_word_RL_8by4/val_count)\n",
    "    print('count: ', val_count)\n",
    "\n",
    "    print()\n",
    "\n",
    "prob_bi = np.zeros((testDataset.__len__(), 4))\n",
    "prob_bi[:, 0] = np.array(prob_list_LR_4by8)\n",
    "prob_bi[:, 1] = np.array(prob_list_RL_4by8)\n",
    "prob_bi[:, 2] = np.array(prob_list_LR_8by4)\n",
    "prob_bi[:, 3] = np.array(prob_list_RL_8by4)\n",
    "\n",
    "pred_bool_bi = np.zeros((testDataset.__len__(), 4))\n",
    "pred_bool_bi[:, 0] = temp_pred_bool_arr_LR_4by8\n",
    "pred_bool_bi[:, 1] = temp_pred_bool_arr_RL_4by8 \n",
    "pred_bool_bi[:, 2] = temp_pred_bool_arr_LR_8by4 \n",
    "pred_bool_bi[:, 3] = temp_pred_bool_arr_RL_8by4 \n",
    "\n",
    "arg_prob_bi = np.argmax(prob_bi, axis=1)\n",
    "\n",
    "print('Highest Prob in LR 4by8:' ,np.sum(arg_prob_bi == 0))\n",
    "print('Highest Prob in RL 4by8:' ,np.sum(arg_prob_bi == 1))\n",
    "print('Highest Prob in LR 8by4:' ,np.sum(arg_prob_bi == 2))\n",
    "print('Highest Prob in RL 8by4:' ,np.sum(arg_prob_bi == 3))\n",
    "\n",
    "max_prob_pred_bool = pred_bool_bi[np.arange(len(prob_list_LR_4by8)), arg_prob_bi]\n",
    "print('Acc:', np.sum(max_prob_pred_bool)/len(prob_list_LR_4by8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16f0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STR",
   "language": "python",
   "name": "str"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
